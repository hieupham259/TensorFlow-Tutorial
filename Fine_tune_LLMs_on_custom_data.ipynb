{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1cb5e1d6",
      "metadata": {
        "id": "1cb5e1d6"
      },
      "source": [
        "# **Finetune LLMs for Free: A Step-by-Step Open-Source Tutorial**\n",
        "\n",
        "**Welcome to this tutorial!** In the following steps, we will show you how to fine-tune the llama-2 7B LLM for **Question Answering** on your own private data. Simply run the cells sequentially, to learn how to load the model and your data, train the model, and finally ask questions about your data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why would this tutorial be helpful for you?\n",
        "\n",
        "Our tutorial to fine-tune llama 2 uniquely offers several advantages over others currently out in the internet, namely:\n",
        "\n",
        "1. The ability to **fine-tune on freeform text articles and documents**; we don't assume you manually format your data in well defined formats like CSV, JSON, etc.\n",
        "\n",
        "2. Offer **greater degree of control** and **in-depth explanations** to users on tuning parameters to **optimize performance of fine-tuned model** specific for your use case\n",
        "\n",
        "3. Provide a **walkthrough of fine-tuning Llama-2 with high accuracy and demonstration** of this model based on a **realistic use case**."
      ],
      "metadata": {
        "id": "unHNswo93hd8"
      },
      "id": "unHNswo93hd8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Authors and Credits\n",
        "\n",
        "[Sri Ranganathan Palaniappan](https://www.linkedin.com/in/sri-ranganathan-palaniappan/), CS undergrad student at Georgia Tech.\n",
        "\n",
        "[Mansi Phute](https://www.linkedin.com/in/mansi-phute-413744166/),  CS masters student at Georgia Tech\n",
        "\n",
        "[Seongmin Lee](https://www.linkedin.com/in/seongmin-lee-8b8a97209/),  CS PhD student at Georgia Tech\n",
        "\n",
        "[Polo Chau](https://www.linkedin.com/in/polochau/),  Associate Professor at Georgia Tech."
      ],
      "metadata": {
        "id": "Dw5v8Qjt5uFs"
      },
      "id": "Dw5v8Qjt5uFs"
    },
    {
      "cell_type": "markdown",
      "id": "ebe583d2",
      "metadata": {
        "id": "ebe583d2"
      },
      "source": [
        "**Memory requirements: How many GPU(s) to use?**\n",
        "\n",
        "This tutorial needs no more than 14GB GPU RAM. Depending on the GPU you’re using, one GPU may be sufficient (e.g., NVIDIA RTX 4090 has 24GB RAM). Modify the code below to specify the number of GPUs to use.\n",
        "\n",
        "Refer to this [medium post](https://medium.com/polo-club-of-data-science/memory-requirements-for-fine-tuning-llama-2-80f366cba7f5) to learn more about the memory requirements for finetuning Llama-2-7B!\n",
        "\n",
        "**Note: If you're using Google Colab, the code below automatically sets the GPU to T4.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fce1cf0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fce1cf0b",
        "outputId": "5af24be8-1e83-4383-8fb8-a0e55793893e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: GPUtil in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% |  0% |\n",
            "GPU is available!\n",
            "Tue Dec 23 07:49:56 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8             11W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!pip install GPUtil\n",
        "\n",
        "import torch\n",
        "import GPUtil\n",
        "import os\n",
        "\n",
        "GPUtil.showUtilization()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available!\")\n",
        "else:\n",
        "    print(\"GPU not available.\")\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "\n",
        "# Change if needed to accomodate memory requirements!\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to the GPU ID (0 for T4)\n",
        "\n",
        "!nvidia-smi # Verify notebook is running on a GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OMhFiK5e3zGM",
      "metadata": {
        "id": "OMhFiK5e3zGM"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "Execute the cell below to import the necessary libraries for fine-tuning Llama-2.\n",
        "\n",
        "**Note:** *You only need to run this cell once initially, and you can then comment it out.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "64DI5Br5dFgJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64DI5Br5dFgJ",
        "outputId": "a921a06f-a416-40c9-baf9-eaf06906a253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig,LlamaTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "from datasets import load_dataset\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datetime import datetime\n",
        "\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d97d87f8",
      "metadata": {
        "id": "d97d87f8",
        "outputId": "bfb255da-42d9-46f6-cb92-faabf35910a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `hf auth whoami` to get more information or `hf auth logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: write).\n",
            "The token `ANIME RECOMMENDER` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `ANIME RECOMMENDER`\n"
          ]
        }
      ],
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "    !huggingface-cli login\n",
        "else:\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "71e6b71d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b43398e7ecac45268020c866d51d49d4",
            "f7052af5653047cf88f83bc47c263420",
            "4f57f429fd2240c09dad96c9b4341518",
            "5721c1b42efc49a5a2aba26861b0a7ce",
            "487b004768704103a4fc23b0bc2b29bf",
            "7f3899b148924a9c9298e8c706d68893",
            "dff032b1177044939aefa6a2c9c27270",
            "7b9e1bfd35b941b7882128c219a12db4",
            "1311d64b8fc14329aac91807fcba64e4",
            "7bbd7f4cccc344c2aceb46a5ff004e0e",
            "4857dc49e06e4b28b35a15c9e2594a84"
          ]
        },
        "id": "71e6b71d",
        "outputId": "900c15ad-f3bd-4da1-f22f-16e3e1108751"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b43398e7ecac45268020c866d51d49d4"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
          ]
        }
      ],
      "source": [
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "'''\n",
        "NOTE:\n",
        "\n",
        "\n",
        "load_in_4bit: Enables loading the model using 4-bit quantization, reducing\n",
        "memory and computational costs.\n",
        "\n",
        "bnb_4bit_compute_dtype: Sets the computational data type for the 4-bit quantized\n",
        "model, controlling precision during inference or training.\n",
        "'''\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
        "                                             quantization_config=bnb_config)\n",
        "print(model.__class__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7716e73a",
      "metadata": {
        "id": "7716e73a"
      },
      "source": [
        "## Loading datasets for training\n",
        "\n",
        "<!-- For fine-tuning Llama-2-7b-chat model on your own private data, depending on the usecases, the format of your input data plays a significant role in the training process. In this guide, we'll explore two common formats for organizing your data: **JSON for structured data and text (.txt) for unstructured data**. -->\n",
        "\n",
        "<!-- ### **Option 1: Structured Data (JSON Format)**\n",
        "\n",
        "JSON formatted input data provides a structured format consisting of question-answer pairs. Each datapoint (each line) consists of a question and its corresponding answer, allowing for clear and explicit mapping of question-answer pairs. This format is ideal for scenarios where data is neatly organized and follows a predictable structure.\n",
        "\n",
        "**Pros:**\n",
        "- Clear and organized structure, resulting in better training results.\n",
        "- Explicit mapping for question with corresponding answer.\n",
        "- Suitable for datasets with consistent formatting.\n",
        "\n",
        "**Cons:**\n",
        "- Labor intensive: requires additional processing to convert unstructured data into a structured format.\n",
        "- Less flexible for datasets with varying or irregular structures. -->\n",
        "\n",
        "<!-- ### **Loading data as multiple .txt files** -->\n",
        "### **Unstructured Data (Text Format)**\n",
        "\n",
        "**Pros:**\n",
        "- Flexible with diverse data formats with irregular structures, not limited to Question-Answer pairs.\n",
        "- Can contain blocks of text from guides/articles or multiple Q&A pairs.\n",
        "- Simple to create and modify; offers enhanced readability and reduced labor intensity.\n",
        "\n",
        "**Cons:**\n",
        "- Compared to JSON format, offers less structured data, potentially resulting in inferior training outcomes.\n",
        "\n",
        "\n",
        "All of our training data are formatted as **multiple .txt files**.\n",
        "\n",
        "\n",
        "`train_dataset`: *Important for model training, this dataset contains the training data used to update the model’s weights through backpropagation.*\n",
        "\n",
        "We have provided example training .txt files cloned from a github repository that shows how your private data may be formatted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7e7bb915",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e7bb915",
        "outputId": "0ed01be1-719b-4ab9-8579-7a47a00d8922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Fine-tuning-LLMs' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/poloclub/Fine-tuning-LLMs.git\n",
        "\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    dir_path = '/content/Fine-tuning-LLMs/data/'\n",
        "else:\n",
        "    dir_path = 'Fine-tuning-LLMs/data/'\n",
        "\n",
        "os.chdir(dir_path)\n",
        "\n",
        "train_dataset = load_dataset(\"text\", data_files={\"train\":\n",
        "                [\"hawaii_wf_1.txt\", \"hawaii_wf_2.txt\",\n",
        "                 \"hawaii_wf_3.txt\",\"hawaii_wf_4.txt\",\n",
        "                 \"hawaii_wf_5.txt\",\"hawaii_wf_6.txt\",\n",
        "                 \"hawaii_wf_7.txt\",\"hawaii_wf_8.txt\",\n",
        "                \"hawaii_wf_9.txt\",\"hawaii_wf_10.txt\",\"hawaii_wf_11.txt\"]}, split='train')\n",
        "\n",
        "os.chdir('..')\n",
        "os.chdir('..')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "364ab436",
      "metadata": {
        "id": "364ab436"
      },
      "source": [
        "### **Initialize & configure tokenizer**\n",
        "\n",
        "We have set `add_eos_token = True` so that the model knows how to recognize the “end of sentence\". We have also added a special pad token `'[PAD]'` to pad shorter lines to match the length of longer ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4317aea0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4317aea0",
        "outputId": "f15f8fcd-3c48-444c-be84-0ff2dddec937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n"
          ]
        }
      ],
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n",
        "                                           trust_remote_code=True,\n",
        "                                           add_eos_token=True)\n",
        "print(tokenizer.__class__)\n",
        "\n",
        "'''\n",
        "We initialize the Llama tokenizer (slow) for the Llama-2-7b-chat model.\n",
        "The Llama tokenizer is known to have issues with automatically setting\n",
        "the End-of-sentence (eos) token and the padding (pad) token.\n",
        "'''\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# set the pad token to indicate that it's the end-of-sentence\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "710d1a5d",
      "metadata": {
        "id": "710d1a5d"
      },
      "source": [
        "### **Tokenize Prompt for formatted data**\n",
        "\n",
        "We tokenize—converting text into numbers to make it understandable for LLMs—our training dataset using the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "386307ed",
      "metadata": {
        "id": "386307ed"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset=[]\n",
        "for phrase in train_dataset:\n",
        "    tokenized_train_dataset.append(tokenizer(phrase['text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da776488",
      "metadata": {
        "id": "da776488"
      },
      "source": [
        "### ***Bonus Option: Structured data (JSONL format)***\n",
        "\n",
        "*You can also finetune your model on structured data which is formatted as JSONL files! Each datapoint (each line) consists of a question and its corresponding answer, allowing for clear and explicit mapping of question-answer pairs, thus leading to **better training results**.*\n",
        "\n",
        "*Each line is a datapoint that is formatted as:*\n",
        "\n",
        "```json\n",
        "{'Question': 'Your question goes here', 'Answer': 'Your answer goes here'}\n",
        "```\n",
        "\n",
        "*However, this format is **labor intensive and only ideal for scenarios where data is exclusively Q&A pairs** neatly organized and follows a predictable structure.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b7d32b",
      "metadata": {
        "id": "47b7d32b"
      },
      "outputs": [],
      "source": [
        "########## UNCOMMENT CODE BELOW IF INPUT DATA IS JSONL FILES ##########\n",
        "\n",
        "### Load dataset\n",
        "\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# train_dataset = load_dataset(\"json\", data_files={\"train\":\n",
        "#   [\"serviceNowQA_training.jsonl\"]}, split=\"train\")\n",
        "\n",
        "# def formatInputJSONData(datapoint):\n",
        "#     text = f\"{datapoint['Question']}\\n{datapoint['Answer']}\"\n",
        "#     return text\n",
        "\n",
        "### Tokenize data\n",
        "\n",
        "# tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n",
        "#    trust_remote_code=True, add_eos_token=True)\n",
        "\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "### Tokenize prompt\n",
        "\n",
        "# def tokenizePrompt(prompt):\n",
        "#     currPrompt = formatInputJSONData(prompt)\n",
        "#     return tokenizer(currPrompt)\n",
        "\n",
        "# tokenized_train_dataset = train_dataset.map(tokenizePrompt)\n",
        "\n",
        "########## UNCOMMENT CODE ABOVE IF INPUT DATA IS JSONL FILES ##########"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35094e73",
      "metadata": {
        "id": "35094e73"
      },
      "source": [
        "### **PEFT Fine-Tuning**\n",
        "\n",
        "We now prepare the model for knowledge distillation training using the **PEFT (Parameter-Efficient Fine-Tuning)** method to significantly reduce the memory and compute requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0b68fda6",
      "metadata": {
        "id": "0b68fda6"
      },
      "outputs": [],
      "source": [
        "# gradient checkpointing to reduce memory usage for increased compute time\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# compressing the base model into a smaller, more efficient model\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34f755b7",
      "metadata": {
        "id": "34f755b7"
      },
      "source": [
        "### Configure model with LoRA\n",
        "\n",
        "The code below uses [LoRA](https://huggingface.co/docs/diffusers/en/training/lora) (a PEFT method) to reduce the number of trainable parameters. LoRA works by decomposing the large matrix of the pre-trained model into two smaller low-rank matrices in the attention layers which drastically reduces the number of parameters that need to be fine-tuned. Refer to the __[LoRA documentation](https://opendelta.readthedocs.io/en/latest/modules/deltas.html#lora)__ to learn more about the parameters and use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d670cbd1",
      "metadata": {
        "id": "d670cbd1"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    # rank of the update matrices\n",
        "    # Lower rank results in smaller matrices with fewer trainable params\n",
        "    r=8,\n",
        "\n",
        "    # impacts low-rank approximation aggressiveness\n",
        "    # increasing value speeds up training\n",
        "    lora_alpha=64,\n",
        "\n",
        "    # modules to apply the LoRA update matrices\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"down_proj\",\n",
        "        \"up_proj\",\n",
        "        \"o_proj\"\n",
        "    ],\n",
        "\n",
        "    # determines LoRA bias type, influencing training dynamics\n",
        "    bias=\"none\",\n",
        "\n",
        "    # regulates model regularization; increasing may lead to underfitting\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46ac156",
      "metadata": {
        "id": "b46ac156"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "We're now ready to train our Llama 2 model on our new data! We'll be using the [`Transformers`](https://huggingface.co/docs/transformers/en/index) library to create a [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) object for training the model. The `Trainer` takes the pre-trained model, training datasets, training arguments, and data collator as input.\n",
        "\n",
        "Training time depends on the size of the training data, number of epochs and the configuration of the GPU used. If you run the below cell using the sample dataset provided on Google Colab T4 GPU (default), then it should take around **1 hour 30 minutes to complete training** for 3 epochs of the provided sample Hawaii wildfire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e02f0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "06e02f0b",
        "outputId": "5f640c4a-5d0d-4962-dfeb-d49d7fe0dc71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/content/wandb/offline-run-20251223_082649-kwmgr08f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1429' max='1455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1429/1455 1:46:18 < 01:56, 0.22 it/s, Epoch 2.94/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.339400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.871600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.940300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.833500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.916200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.891700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.407400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.674500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.447900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.820800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.664300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.746500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.649300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.579800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.368200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.544300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.552500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.276700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.530700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.517000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.518900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>2.572500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.239300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>2.587200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>2.374300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>2.433300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>2.366100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.413100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>2.294100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>2.499200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>2.499300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>2.363200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.514400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>2.345400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>2.647800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>2.451000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>2.466800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.433600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>2.252000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>2.501600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>2.492900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>2.243700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.301500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>2.479800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>2.681500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>2.284200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>2.090000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.824200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>1.913500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.861300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>2.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.765400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>1.909900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>1.866700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.961100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>1.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.767400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>2.111200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>1.922400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>1.953200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>1.863300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.915700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>1.824900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>1.933900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>1.718800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>1.718600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.926800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>1.879500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>1.834800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>1.903800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>1.856200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.764100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>1.886500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>1.675900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>1.722300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>1.902200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.771500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>1.850100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>1.903200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>1.796600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>1.921000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.759000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>2.104800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>1.690300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>1.609400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>1.682200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>1.705900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>1.665200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>1.586700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>1.792500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>2.007300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>1.790500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>1.891700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>1.185300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>1.288300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.358300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>1.169800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>1.313000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>1.098600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>1.115600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.279000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>1.067300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>1.341000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>1.211200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>0.933800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.106400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>1.056400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>1.176500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>1.157400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>1.149400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.218500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>1.089800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>1.035500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>1.147000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>1.136900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.247400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>1.213700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>1.127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>1.029900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>1.130200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>1.211900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>1.121800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>1.166600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>1.141100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>1.103100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.207900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>1.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>1.178800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>1.166900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>1.203600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>1.191400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>1.170600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>1.255500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.996900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>1.259500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.257800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>1.229200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>1.006500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "### TWO IMPORTANT TRAINING PARAMETERS TO CONSIDER CHANGING\n",
        "\n",
        "train_epochs_val = 3 # CHANGE VALUE AS NEEDED HERE!\n",
        "'''\n",
        "train_epochs_val is the times the model will iterate over the entire training\n",
        "dataset. Increasing the value may allow the model to learn more from the data,\n",
        "but be cautious of overfitting.\n",
        "'''\n",
        "\n",
        "learning_rate_val = 1e-4 # CHANGE VALUE AS NEEDED HERE!\n",
        "'''\n",
        "A higher learning_rate_val can lead to faster convergence, but it might\n",
        "overshoot the optimal solution. Conversely, a lower value may result\n",
        "in slower training but better fine-tuning.\n",
        "'''\n",
        "\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,                             # llama-2-7b-chat model\n",
        "    train_dataset=tokenized_train_dataset,   # training data that's tokenized\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=\"./finetunedModel\",       # directory where checkpoints are saved\n",
        "        per_device_train_batch_size=2,       # number of samples processed in one forward/backward pass per GPU\n",
        "        gradient_accumulation_steps=2,       # [default = 1] number of updates steps to accumulate the gradients for\n",
        "        num_train_epochs=train_epochs_val,   # [IMPORTANT] number of times of complete pass through the entire training dataset\n",
        "        learning_rate=learning_rate_val,     # [IMPORTANT] smaller LR for better finetuning\n",
        "        bf16=False,                          # train parameters with this precision\n",
        "        optim=\"paged_adamw_8bit\",            # use paging to improve memory management of default adamw optimizer\n",
        "        logging_dir=\"./logs\",                # directory to save training log outputs\n",
        "        save_strategy=\"epoch\",               # [default = \"steps\"] store after every iteration of a datapoint\n",
        "        save_steps=50,                       # save checkpoint after number of iterations\n",
        "        logging_steps = 10                   # specify frequency of printing training loss data\n",
        "    ),\n",
        "\n",
        "    # use to form a batch from a list of elements of train_dataset\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# if use_cache is True, past key values are used to speed up decoding\n",
        "# if applicable to model. This defeats the purpose of finetuning\n",
        "model.config.use_cache = False\n",
        "\n",
        "# train the model based on the above config\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a323fd2",
      "metadata": {
        "id": "7a323fd2"
      },
      "source": [
        "## **Load Finetuned Model**\n",
        "\n",
        "Congratulations! You've successfully finetuned the Llama 2 model on your data. Now, let's load the finetuned model using the `BitsAndBytesConfig` we used previously.\n",
        "\n",
        "Ensure to choose the model checkpoint with the least training loss (as seen in the training output table above).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dc17744",
      "metadata": {
        "id": "5dc17744"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig,LlamaTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "nf4Config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n",
        "                                           trust_remote_code=True,\n",
        "                                           add_eos_token=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  #same as before\n",
        "    quantization_config=nf4Config,  #same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n",
        "                                           trust_remote_code=True)\n",
        "\n",
        "# Change model checkpoint that has least training loss in the code below\n",
        "# beware of overfitting!\n",
        "modelFinetuned = PeftModel.from_pretrained(base_model,\"finetunedModel/checkpoint-1455\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uyFprt6yv1H-",
      "metadata": {
        "id": "uyFprt6yv1H-"
      },
      "source": [
        "### **Ask questions to your finetuned model!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e143ea",
      "metadata": {
        "id": "95e143ea"
      },
      "source": [
        "#### Option 1: Chatbot with Predefined Prompts\n",
        "\n",
        "Enter your question as a string and assign it to the variable `question`.\n",
        "\n",
        "*Note: Feel free to duplicate the below cell for new questions to keep a history of the chat.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4158861",
      "metadata": {
        "id": "a4158861",
        "outputId": "9e1e3b5d-f24c-4c95-92b0-79cf826b6778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Just answer this question: Tell me about the role of Maui Emergency Management Agency (MEMA) in the 2023 wildfires??\n",
            "\n",
            "MEMA played a crucial role in coordinating and facilitating the response to the 2023 wildfires. MEMA’s primary role is to coordinate and facilitate the response to emergencies and disasters in Maui County. In the case of the 2023 wildfires, MEMA’s responsibilities included:\n",
            "\n",
            "1. Coordinating with other emergency response agencies, such as the Maui Police Department, Maui Fire Department, and the Hawaii National Guard, to ensure a unified response.\n",
            "2. Activating the Maui Emergency Operations Center (EOC), which served as the centralized location for coordination of response and support activities.\n",
            "3. Providing situational reports and information to the County of Maui Mayor’s office and other stakeholders.\n",
            "4. Coordinating with media outlets to ensure accurate and timely dissemination of information to the public.\n",
            "5. Managing and coordinating volunteer and donated resources.\n",
            "6. Coordinating with other government agencies, such as the Federal Emergency Management Agency (FEMA), the American Red Cross, and other support agencies.\n",
            "7. Providing support to emergency shelters and other response efforts.\n",
            "8. Coordinating with the Maui County Corporation Counsel’s office to ensure legal support for emergency response and recovery efforts.\n",
            "9. Coordinating with the Maui Police Department’s Wailuku and Lanai districts to ensure support for the Wailuku and Lanai communities.\n",
            "10. Coordinating with the Maui Fire Department’s Wailuku and Lanai stations to ensure support for the Wailuku and Lanai fire units.\n",
            "\n",
            "In summary, MEMA played a crucial role in coordinating and facilitating the response to the 2023 wildfires in Maui County. MEMA’s responsibilities included coordinating with other emergency response agencies, activating the Maui EOC, providing situational reports and information, managing and coordinating volunteer and donated resources, and providing support to emergency shelters and other response efforts.\n"
          ]
        }
      ],
      "source": [
        "### ENTER YOUR QUESTION BELOW\n",
        "\n",
        "question = \"Just answer this question: Tell me about the role of Maui Emergency Management Agency (MEMA) in the 2023 wildfires??\"\n",
        "\n",
        "# Format the question\n",
        "eval_prompt = f\"{question}\\n\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de4a843e",
      "metadata": {
        "id": "de4a843e"
      },
      "source": [
        "#### Option 2: Interactive chatbot!\n",
        "\n",
        "Enter your question in the textbox that pops up after running the cell below!\n",
        "\n",
        "*Note: Feel free to duplicate the below cell for new questions to keep a history of the chat.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42219f0b",
      "metadata": {
        "id": "42219f0b",
        "outputId": "34dcab95-84dd-4576-d4a3-f38cf0417679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: Give me a brief summary on the Hawaii wildfires in 2023 in about 150 words?\n",
            "Just answer this question concisely: Give me a brief summary on the Hawaii wildfires in 2023 in about 150 words?\n",
            "\n",
            "The Hawaii wildfires in 2023 were a series of fires that occurred across the state, particularly in the Kula and Olinda regions of Maui. The fires started on August 8, 2023 and continued for several days, resulting in the deaths of two people and the destruction of hundreds of homes. The fires were fueled by strong winds and dry conditions, and were ultimately contained by firefighters on August 10, 2023. The total area burned was approximately 2,170 acres, with 300 acres of that being in the Kula fire. (LAHAINA)\n"
          ]
        }
      ],
      "source": [
        "### RUN THIS CELL AND ENTER YOUR QUESTION IN THE POP-UP BOX\n",
        "\n",
        "# User enters question below\n",
        "user_question = input(\"Enter your question: \")\n",
        "\n",
        "# Format the question\n",
        "eval_prompt = f\"Just answer this question concisely: {user_question}\\n\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c09a19",
      "metadata": {
        "id": "48c09a19",
        "outputId": "849be72e-ebb0-4356-e33f-7ae8acdac0d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Just answer this question accurately: Summarize the officer accounts of the wildfire in Hawaii?\n",
            "\n",
            "Answer: The officer accounts of the wildfire in Hawaii are that the fire was severe and rapidly moving, with high winds and zero visibility. Officers reported the fire as being out of control and in some areas, the fire was moving faster than officers could evacuate. Officers reported the fire as being in areas they could not physically reach with water, and the fire was engulfing everything in its path. Officers reported the fire as a life-threatening situation and evacuations were being conducted as quickly as possible. Officers reported the fire as a large scale incident that would take days to contain and control. Officers reported the fire as a tragic event that resulted in loss of life and extensive damage to property. Officers reported the fire as a reminder of the dangers of wildfires and the importance of being prepared and having a plan in place. Officers reported the fire as a challenging and dangerous situation, but one that they were determined to control and contain. Officers reported the fire as a success in terms of evacuations and containment, but a tragic loss of life and property. Officers reported the fire as a reminder of the importance of community and teamwork in overcoming adversity. Officers reported the fire as a challenging and dangerous situation, but one that they were determined to overcome.\n"
          ]
        }
      ],
      "source": [
        "# User enters question below\n",
        "user_question = \"Summarize the officer accounts of the wildfire in Hawaii?\"\n",
        "\n",
        "# Format the question\n",
        "eval_prompt = f\"Just answer this question accurately: {user_question}\\n\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87285606",
      "metadata": {
        "id": "87285606",
        "outputId": "fd9a8093-c583-4806-ddae-9d7b91edde09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Just answer this question in less than 300 words: Tell me about the media coverage of the hawaii wildfires?\n",
            "\n",
            "Media coverage of the Hawaii wildfires was extensive, with both domestic and international media outlets covering the events. Local news outlets, such as Hawaii News Now and KGMB, provided continuous coverage of the fires, including live reports and interviews with experts and those affected. National news outlets, like CNN and Fox News, also provided extensive coverage, with correspondents and reporters traveling to Hawaii to report on the fires. International media outlets, such as the BBC and Al Jazeera, also covered the fires. Social media was also used to disseminate information, with the National Park Service and other government agencies using Twitter and Facebook to provide updates and information. Overall, the media coverage of the Hawaii wildfires was extensive and widespread, providing important information and updates to the public.\n"
          ]
        }
      ],
      "source": [
        "# User enters question below\n",
        "user_question = \"Tell me about the media coverage of the hawaii wildfires?\"\n",
        "\n",
        "# Format the question\n",
        "eval_prompt = f\"Just answer this question in less than 300 words: {user_question}\\n\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34f35160",
      "metadata": {
        "id": "34f35160",
        "outputId": "16c5124c-a3e9-4daf-99f6-928c5eb5b7fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Just answer this question concisely: Where were the media briefings held?\n",
            "\n",
            "Answer: The media briefings were held at the Lahaina Civic Center.\n"
          ]
        }
      ],
      "source": [
        "# User enters question below\n",
        "user_question = \"Where were the media briefings held?\"\n",
        "\n",
        "# Format the question\n",
        "eval_prompt = f\"Just answer this question concisely: {user_question}\\n\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b0b891",
      "metadata": {
        "id": "03b0b891",
        "outputId": "6ee82cea-20a6-4eeb-8a4e-4e3137fd002f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Tell me about Morgue Identification and Notification Task Force (M.I.N.T)?. Just answer this question accurately\n",
            "\n",
            "Answer: The Morgue Identification and Notification Task Force (M.I.N.T) is a team of experts from the FBI, Honolulu Police Department, and the Hawaii State Medical Examiner's Office created to investigate cold cases involving missing persons and unidentified decedents. The team was created in 2009 and has since helped to identify over 100 decedents and reunite families with their loved ones. The team uses advanced technology and forensic science to investigate and match missing persons to decedents. The team also works closely with the FBI's National Missing and Unidentified Persons System (NamUs) to help identify and reunite families across the country.\n"
          ]
        }
      ],
      "source": [
        "# User enters question below\n",
        "user_question = \"Tell me about Morgue Identification and Notification Task Force (M.I.N.T)?\"\n",
        "\n",
        "# Format the question\n",
        "eval_prompt = f\"Question: {user_question}. Just answer this question accurately and concisely\\n\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "052aafad",
      "metadata": {
        "id": "052aafad",
        "outputId": "524cf883-1af8-40a2-fdc4-967eab907908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: When did the Hawaii wildfires take place?. Just answer this question accurately\n",
            "\n",
            "Answer: The Hawaii wildfires took place from August 8, 2023 to August 12, 2023.\n"
          ]
        }
      ],
      "source": [
        "# User enters question below\n",
        "user_question = \"When did the Hawaii wildfires take place?\"\n",
        "\n",
        "# Format the question\n",
        "eval_prompt = f\"Question: {user_question}. Just answer this question accurately and concisely\\n\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074571b3",
      "metadata": {
        "id": "074571b3",
        "outputId": "a5baf77e-e1b5-4592-8335-3d7f3df9a54c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Just answer this question concisely: Which geographic areas were affected by the wildfires in Hawaii in 2023? Give me some insights on this please?\n",
            "\n",
            "The wildfires in Hawaii in 2023 affected the following geographic areas:\n",
            "\n",
            "1. Lahaina and nearby areas on the western side of Maui.\n",
            "2. Kula and nearby areas on the northern side of Maui.\n",
            "3. Kihei and nearby areas on the southern side of Maui.\n",
            "4. Molokai, where the fire was the largest and most extensive.\n",
            "5. The Big Island, where the fire was the most destructive.\n",
            "\n",
            "These fires occurred between August 8, 2023 and August 12, 2023.\n"
          ]
        }
      ],
      "source": [
        "# User enters question below\n",
        "user_question = \"Which geographic areas were affected by the wildfires in Hawaii in 2023? Give me some insights on this please?\"\n",
        "\n",
        "# Format the question\n",
        "eval_prompt = f\"Just answer this question concisely and accurately: {user_question}\\n\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "79FV53rqN1mn"
      },
      "id": "79FV53rqN1mn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YiZQbKMXOF5Y"
      },
      "id": "YiZQbKMXOF5Y",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b43398e7ecac45268020c866d51d49d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7052af5653047cf88f83bc47c263420",
              "IPY_MODEL_4f57f429fd2240c09dad96c9b4341518",
              "IPY_MODEL_5721c1b42efc49a5a2aba26861b0a7ce"
            ],
            "layout": "IPY_MODEL_487b004768704103a4fc23b0bc2b29bf"
          }
        },
        "f7052af5653047cf88f83bc47c263420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f3899b148924a9c9298e8c706d68893",
            "placeholder": "​",
            "style": "IPY_MODEL_dff032b1177044939aefa6a2c9c27270",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4f57f429fd2240c09dad96c9b4341518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b9e1bfd35b941b7882128c219a12db4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1311d64b8fc14329aac91807fcba64e4",
            "value": 2
          }
        },
        "5721c1b42efc49a5a2aba26861b0a7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bbd7f4cccc344c2aceb46a5ff004e0e",
            "placeholder": "​",
            "style": "IPY_MODEL_4857dc49e06e4b28b35a15c9e2594a84",
            "value": " 2/2 [01:02&lt;00:00, 28.60s/it]"
          }
        },
        "487b004768704103a4fc23b0bc2b29bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f3899b148924a9c9298e8c706d68893": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff032b1177044939aefa6a2c9c27270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b9e1bfd35b941b7882128c219a12db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1311d64b8fc14329aac91807fcba64e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7bbd7f4cccc344c2aceb46a5ff004e0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4857dc49e06e4b28b35a15c9e2594a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}