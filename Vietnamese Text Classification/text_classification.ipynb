{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bdd112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Tensorflow-Tutorial\\venv\\Lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Tensorflow-Tutorial\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d45f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Má»t trong nhá»¯ng ngÆ°á»i ÄÃ¡nh giÃ¡ khÃ¡c ÄÃ£ Äá» cáº­p ráº±ng sau khi xem chá» 1 táº­p Oz, báº¡n sáº½ bá» cuá»n hÃºt. Há» nÃ³i ÄÃºng, vÃ¬ ÄÃ¢y chÃ­nh xÃ¡c lÃ  nhá»¯ng gÃ¬ ÄÃ£ xáº£y ra vá»i tÃ´i.<br /><br />Äiá»u Äáº§u tiÃªn khiáº¿n tÃ´i áº¥n tÆ°á»£ng vá» Oz lÃ  sá»± tÃ n báº¡o vÃ  nhá»¯ng cáº£nh báº¡o lá»±c khÃ´ng há» nao nÃºng, thá» hiá»n ngay tá»« chá»¯ ÄI. Tin tÃ´i Äi, ÄÃ¢y khÃ´ng pháº£i lÃ  má»t chÆ°Æ¡ng trÃ¬nh dÃ nh cho nhá»¯ng ngÆ°á»i yáº¿u tim hay nhÃºt nhÃ¡t. ChÆ°Æ¡ng trÃ¬nh nÃ y khÃ´ng cÃ³ cÃº Äáº¥m nÃ o liÃªn quan Äáº¿n ma tÃºy, tÃ¬nh dá»¥c hay báº¡o lá»±c. NÃ³ lÃ  háº¡ng náº·ng, theo cÃ¡ch sá»­ dá»¥ng cá» Äiá»n cá»§a tá»« nÃ y.<br /><br />NÃ³ ÄÆ°á»£c gá»i lÃ  OZ vÃ¬ ÄÃ³ lÃ  biá»t danh ÄÆ°á»£c Äáº·t cho Tráº¡i giam NhÃ  nÆ°á»c An ninh Tá»i Äa Oswald. NÃ³ táº­p trung chá»§ yáº¿u vÃ o ThÃ nh phá» Ngá»c lá»¥c báº£o, má»t khu vá»±c thá»­ nghiá»m cá»§a nhÃ  tÃ¹ nÆ¡i táº¥t cáº£ cÃ¡c phÃ²ng giam Äá»u cÃ³ máº·t trÆ°á»c báº±ng kÃ­nh vÃ  hÆ°á»ng vÃ o trong, vÃ¬ váº­y quyá»n riÃªng tÆ° khÃ´ng ÄÆ°á»£c Äá» cao trong chÆ°Æ¡ng trÃ¬nh nghá» sá»±. ThÃ nh phá» Em lÃ  nÆ¡i sinh sá»ng cá»§a nhiá»u ngÆ°á»i..NgÆ°á»i Aryan, ngÆ°á»i Há»i giÃ¡o, xÃ£ há»i Äen, ngÆ°á»i Latinh, ngÆ°á»i theo Äáº¡o CÆ¡ Äá»c, ngÆ°á»i Ã, ngÆ°á»i Ireland, v.v.... nÃªn nhá»¯ng vá»¥ áº©u Äáº£, nhá»¯ng cÃ¡i nhÃ¬n cháº±m cháº±m cháº¿t chÃ³c, nhá»¯ng giao dá»ch ma mÃ£nh vÃ  nhá»¯ng thá»a thuáº­n má» Ã¡m khÃ´ng bao giá» xa vá»i.<br /><br / > TÃ´i muá»n nÃ³i ráº±ng sá»©c háº¥p dáº«n chÃ­nh cá»§a chÆ°Æ¡ng trÃ¬nh lÃ  do nÃ³ Äi Äáº¿n nhá»¯ng nÆ¡i mÃ  cÃ¡c chÆ°Æ¡ng trÃ¬nh khÃ¡c khÃ´ng dÃ¡m. HÃ£y quÃªn Äi nhá»¯ng bá»©c tranh Äáº¹p Äáº½ ÄÆ°á»£c váº½ cho khÃ¡n giáº£ phá» thÃ´ng, quÃªn Äi sá»± quyáº¿n rÅ©, quÃªn Äi sá»± lÃ£ng máº¡n... OZ khÃ´ng gÃ¢y rá»i. Táº­p Äáº§u tiÃªn tÃ´i tá»«ng xem gÃ¢y cho tÃ´i cáº£m giÃ¡c kinh tá»m Äáº¿n má»©c siÃªu thá»±c, tÃ´i khÃ´ng thá» nÃ³i ráº±ng mÃ¬nh ÄÃ£ sáºµn sÃ ng cho nÃ³, nhÆ°ng khi xem nhiá»u hÆ¡n, tÃ´i báº¯t Äáº§u thÃ­ch Oz vÃ  quen vá»i nhá»¯ng hÃ¬nh áº£nh báº¡o lá»±c á» má»©c Äá» cao. KhÃ´ng chá» lÃ  báº¡o lá»±c, mÃ  cÃ²n lÃ  sá»± báº¥t cÃ´ng (nhá»¯ng tÃªn cai ngá»¥c quanh co sáº½ bá» bÃ¡n Äá»©ng vÃ¬ má»t Äá»ng xu, nhá»¯ng tÃ¹ nhÃ¢n sáº½ giáº¿t ngÆ°á»i theo lá»nh vÃ  bá» trá»n, nhá»¯ng tÃ¹ nhÃ¢n trung lÆ°u, lá»ch sá»± bá» biáº¿n thÃ nh nhá»¯ng con chÃ³ cÃ¡i trong tÃ¹ do thiáº¿u ká»¹ nÄng ÄÆ°á»ng phá» hoáº·c tráº£i nghiá»m trong tÃ¹) Xem Oz, báº¡n cÃ³ thá» trá» nÃªn thoáº£i mÃ¡i vá»i nhá»¯ng gÃ¬ khÃ´ng thoáº£i mÃ¡i khi xem.... ÄÃ³ lÃ  náº¿u báº¡n cÃ³ thá» tiáº¿p xÃºc vá»i máº·t tá»i cá»§a mÃ¬nh.\n",
      "Một trong những người đánh giá khác đã đề cập rằng sau khi xem chỉ 1 tập Oz, bạn sẽ bị cuốn hút. Họ nói đúng, vì đây chính xác là những gì đã xảy ra với tôi.<br /><br />Điều đầu tiên khiến tôi ấn tượng về Oz là sự tàn bạo và những cảnh bạo lực không hề nao núng, thể hiện ngay từ chữ ĐI. Tin tôi đi, đây không phải là một chương trình dành cho những người yếu tim hay nhút nhát. Chương trình này không có cú đấm nào liên quan đến ma túy, tình dục hay bạo lực. Nó là hạng nặng, theo cách sử dụng cổ điển của từ này.<br /><br />Nó được gọi là OZ vì đó là biệt danh được đặt cho Trại giam Nhà nước An ninh Tối đa Oswald. Nó tập trung chủ yếu vào Thành phố Ngọc lục bảo, một khu vực thử nghiệm của nhà tù nơi tất cả các phòng giam đều có mặt trước bằng kính và hướng vào trong, vì vậy quyền riêng tư không được đề cao trong chương trình nghị sự. Thành phố Em là nơi sinh sống của nhiều người..Người Aryan, người Hồi giáo, xã hội đen, người Latinh, người theo đạo Cơ đốc, người Ý, người Ireland, v.v.... nên những vụ ẩu đả, những cái nhìn chằm chằm chết chóc, những giao dịch ma mãnh và những thỏa thuận mờ ám không bao giờ xa vời.<br /><br / > Tôi muốn nói rằng sức hấp dẫn chính của chương trình là do nó đi đến những nơi mà các chương trình khác không dám. Hãy quên đi những bức tranh đẹp đẽ được vẽ cho khán giả phổ thông, quên đi sự quyến rũ, quên đi sự lãng mạn... OZ không gây rối. Tập đầu tiên tôi từng xem gây cho tôi cảm giác kinh tởm đến mức siêu thực, tôi không thể nói rằng mình đã sẵn sàng cho nó, nhưng khi xem nhiều hơn, tôi bắt đầu thích Oz và quen với những hình ảnh bạo lực ở mức độ cao. Không chỉ là bạo lực, mà còn là sự bất công (những tên cai ngục quanh co sẽ bị bán đứng vì một đồng xu, những tù nhân sẽ giết người theo lệnh và bỏ trốn, những tù nhân trung lưu, lịch sự bị biến thành những con chó cái trong tù do thiếu kỹ năng đường phố hoặc trải nghiệm trong tù) Xem Oz, bạn có thể trở nên thoải mái với những gì không thoải mái khi xem.... đó là nếu bạn có thể tiếp xúc với mặt tối của mình.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "\n",
    "def fix_vi_text(text):\n",
    "    try:\n",
    "        text = text.encode(\"latin1\").decode(\"utf-8\")\n",
    "    except:\n",
    "        pass\n",
    "    text = fix_text(text)\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv(\"data/VI_IMDB.csv\", encoding=\"latin1\")\n",
    "print(df[\"vi_review\"].iloc[0])\n",
    "\n",
    "df[\"vi_review\"] = df[\"vi_review\"].astype(str).apply(fix_vi_text)\n",
    "df.to_csv(\"data/VI_IMDB_fixed.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# vefifying the fix\n",
    "df = pd.read_csv(\"data/VI_IMDB_fixed.csv\", encoding=\"utf-8\")\n",
    "print(df[\"vi_review\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3a1762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train folder: E:\\Tensorflow-Tutorial\\Vietnamese Text Classification\\data\\vi_imdb_split\\train\n",
      "Test folder:  E:\\Tensorflow-Tutorial\\Vietnamese Text Classification\\data\\vi_imdb_split\\test\n",
      "Wrote dataset to E:\\Tensorflow-Tutorial\\Vietnamese Text Classification\\data\\vi_imdb_split\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Paths and parameters\n",
    "\n",
    "csv_path = Path(\"data/VI_IMDB_fixed.csv\")\n",
    "\n",
    "output_root = Path(\"data/vi_imdb_split\")\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "\n",
    "\n",
    "def custom_standardization(input_data: tf.Tensor) -> tf.Tensor:\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Load data and pick the Vietnamese review column\n",
    "\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "\n",
    "review_cols = [c for c in df.columns if \"vi_review\" in c]\n",
    "\n",
    "if not review_cols:\n",
    "\n",
    "    raise ValueError(\"No column containing 'vi_review' found in the CSV\")\n",
    "\n",
    "text_col = review_cols[0]\n",
    "\n",
    "if \"sentiment\" not in df.columns:\n",
    "\n",
    "    raise ValueError(\"Column 'sentiment' not found in the CSV\")\n",
    "\n",
    "\n",
    "\n",
    "# Keep only needed columns and drop missing rows\n",
    "\n",
    "df = df[[text_col, \"sentiment\"]].dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Train/test split (stratified so class balance is preserved)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    stratify=df[\"sentiment\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def write_split(split_df: pd.DataFrame, split_name: str) -> Path:\n",
    "    # Standardize text for this split and write one review per txt under pos/neg\n",
    "    standardized = custom_standardization(\n",
    "        tf.constant(split_df[text_col].astype(str).values)\n",
    "    ).numpy()\n",
    "    split_df = split_df.copy()\n",
    "    split_df[text_col] = [s.decode(\"utf-8\", errors=\"ignore\") for s in standardized]\n",
    "\n",
    "    split_dir = output_root / split_name\n",
    "    for label_value, group in split_df.groupby(split_df[\"sentiment\"].str.lower().str.strip()):\n",
    "\n",
    "        if label_value not in {\"positive\", \"negative\"}:\n",
    "\n",
    "            continue\n",
    "\n",
    "        label_dir = split_dir / (\"pos\" if label_value == \"positive\" else \"neg\")\n",
    "\n",
    "        label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for idx, text in enumerate(group[text_col], start=1):\n",
    "\n",
    "            file_path = label_dir / f\"{split_name}_{label_value}_{idx}.txt\"\n",
    "\n",
    "            file_path.write_text(str(text).strip(), encoding=\"utf-8\")\n",
    "    return split_dir\n",
    "\n",
    "\n",
    "\n",
    "train_dir = write_split(train_df, \"train\")\n",
    "\n",
    "test_dir = write_split(test_df, \"test\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train folder: {train_dir.resolve()}\")\n",
    "\n",
    "print(f\"Test folder:  {test_dir.resolve()}\")\n",
    "\n",
    "print(f\"Wrote dataset to {output_root.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "036fe668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40000 files belonging to 2 classes.\n",
      "Using 32000 files for training.\n",
      "Number of batches in the training dataset: 1000\n",
      "Found 40000 files belonging to 2 classes.\n",
      "Using 8000 files for validation.\n",
      "Number of batches in the validation dataset: 250\n",
      "Found 10000 files belonging to 2 classes.\n",
      "Number of batches in the test dataset: 313\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "def setup_dataset(train_dir, test_dir):\n",
    "    batch_size = 32\n",
    "    seed = 42\n",
    "    raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "        train_dir,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=seed,\n",
    "    )\n",
    "    amount_batches = tf.data.experimental.cardinality(raw_train_ds)\n",
    "    print(f'Number of batches in the training dataset: {amount_batches}')\n",
    "\n",
    "    raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "        train_dir,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=seed,\n",
    "    )\n",
    "    amount_batches = tf.data.experimental.cardinality(raw_val_ds)\n",
    "    print(f'Number of batches in the validation dataset: {amount_batches}')\n",
    "\n",
    "    raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "        test_dir,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    amount_batches = tf.data.experimental.cardinality(raw_test_ds)\n",
    "    print(f'Number of batches in the test dataset: {amount_batches}')\n",
    "\n",
    "    return raw_train_ds, raw_val_ds, raw_test_ds\n",
    "\n",
    "raw_train_ds, raw_val_ds, raw_test_ds = setup_dataset(train_dir, test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "565e870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text batch:  32\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "print(\"Length of text batch: \", len(text_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1396addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Use SBERT for vectorization with UTF-8 decoding to avoid ASCII decode issues\n",
    "sbert = SentenceTransformer(\"keepitreal/vietnamese-sbert\")\n",
    "embedding_dim = sbert.get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "def encode_text_batch(text_batch: tf.Tensor) -> tf.Tensor:\n",
    "    # Decode bytes -> utf-8 (ignore errors) then encode with SBERT\n",
    "    sentences = [s.decode(\"utf-8\", errors=\"ignore\") for s in text_batch.numpy().tolist()]\n",
    "    embeddings = sbert.encode(sentences, convert_to_numpy=True, show_progress_bar=False)\n",
    "    return embeddings.astype(\"float32\")\n",
    "\n",
    "\n",
    "def vectorize_dataset(dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
    "    def _map(text, label):\n",
    "        embeddings = tf.py_function(func=encode_text_batch, inp=[text], Tout=tf.float32)\n",
    "        embeddings.set_shape((None, embedding_dim))  # batch, embedding_dim\n",
    "        label = tf.cast(label, tf.float32)\n",
    "        return embeddings, label\n",
    "\n",
    "    return dataset.map(_map, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "vectorized_train_ds = vectorize_dataset(raw_train_ds)\n",
    "vectorized_val_ds = vectorize_dataset(raw_val_ds)\n",
    "vectorized_test_ds = vectorize_dataset(raw_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d18848e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m196,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">213,377</span> (833.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m213,377\u001b[0m (833.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">213,377</span> (833.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m213,377\u001b[0m (833.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# MLP model on SBERT embeddings (shape uses embedding_dim)\n",
    "mlp_model = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(embedding_dim,)),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mlp_model.compile(\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")],\n",
    ")\n",
    "\n",
    "print(mlp_model.summary())\n",
    "\n",
    "# history = mlp_model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=10,\n",
    "#     verbose=1,\n",
    "# )\n",
    "\n",
    "# eval_results = mlp_model.evaluate(test_ds, verbose=1)\n",
    "# print(\"Test results (loss, accuracy, auc):\", eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
